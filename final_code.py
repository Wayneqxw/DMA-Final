# -*- coding: utf-8 -*-
"""Final_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sxsVCYxC8PYn60NBGy9unE1D5WPJzq79

# Final Code

## Import data and packages
"""

#=== pandas
import pandas as pd
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 50)

#=== numpy
import numpy as np

#=== plotting
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

#=== others
import warnings
warnings.filterwarnings("ignore")
import math
from scipy import stats
from datetime import datetime

from google.colab import drive
drive.mount('/content/drive')

!mkdir -p drive
!google-drive-ocamlfuse drive

import os
os.chdir("/content/drive/My Drive/Colab Notebooks/digital_mkt/Final")
print(os.listdir(os.getcwd()))

engagement = pd.read_pickle('engagement')
subscribers = pd.read_pickle('subscribers')
customer_service_reps = pd.read_pickle('customer_service_reps')
advertising_spend = pd.read_excel('advertising_spend_data.xlsx',header=2)

'''
• A subscriber data set, “subscribers”. Each record is a past or current subscriber to your 
service.

• A customer service representative data set “customer_service_reps”. Each record is a 
customer service representative who serves multiple subscribers.

• A product usage data set, “engagement”. Each record is a measure of product 
engagement for a given subscriber on a given date (if the subscriber had any 
engagement on that date).

• Advertising spend, “advertising spend”. Total spent per select channels per month.
'''

"""## Descriptive Analysis

### subscribers
"""

from IPython.display import Image
Image(filename = 'graph/subscriber.jpg', width=1000, height=600)

#========= Look at data
subscribers.reset_index(drop = True, inplace = True)
subscribers

#========= check whether subid is duplicated
sum(subscribers['subid'].value_counts() == 2)

'''
0
'''

#========= check the number of subid
len(subscribers['subid'].unique())

subscribers_id = subscribers['subid']
'''
227628
'''

"""#####cancel related"""

#========= look at paid customers
# 'what is cancel_before_trial_end '??

current_df = subscribers[['subid','package_type','retarget_TF','plan_type',
            'account_creation_date','creation_until_cancel_days','cancel_before_trial_end','trial_end_date',
            'initial_credit_card_declined','revenue_net','paid_TF']]
current_df.head(10)

#========= contradiction in columns 'account_creation_date', 'creation_until_cancel_days', 'cancel_before_trial_end', and 'trial_end_date'

# current_df.loc[(current_df['creation_until_cancel_days'] > 14) & (current_df['cancel_before_trial_end'] == False),'creation_until_cancel_days'].value_counts()
# current_df.loc[(current_df['creation_until_cancel_days'] == 15)]

#==== flip 'cancel_before_trial_end'

current_df['cancel_before_trial_end'] = current_df['cancel_before_trial_end'] == False
current_df

# current_df.loc[current_df['creation_until_cancel_days'] > 14,'cancel_before_trial_end'].value_counts()

#==== check some other weird data
#==== not paid not cancel or paid but cancel
current_df.loc[current_df['cancel_before_trial_end'] == current_df['paid_TF']] # 11507 rows 

# current_df.loc[current_df['cancel_before_trial_end'] == current_df['paid_TF'],'revenue_net'].value_counts() # -> major 0 minor negative

# current_df.loc[current_df['cancel_before_trial_end'] == current_df['paid_TF'],'initial_credit_card_declined'].value_counts()

current_df.loc[current_df['paid_TF'] == True,'cancel_before_trial_end'].value_counts() # all false, seems that paid is good feature

current_df.loc[current_df['paid_TF'] == True]

#====== have to drop some weird data to make data clean
current_df = subscribers[['subid','package_type','retarget_TF','plan_type',
            'account_creation_date','creation_until_cancel_days','cancel_before_trial_end','trial_end_date',
            'initial_credit_card_declined','revenue_net','paid_TF']]
current_df['cancel_before_trial_end'] = current_df['cancel_before_trial_end'] == False

#====== first drop those who just create their account
#==== detect the last day of this dataset
current_df['account_creation_date'].sort_values().iloc[-1] # <- Timestamp('2020-03-27 23:59:06')
#==== suppose this is the last day
#==== drop those whose trial date is later than current date
current_df = current_df.loc[current_df['account_creation_date'] < datetime(2020,4,14)]

#====== next, find some other flaws
current_df.loc[(current_df['cancel_before_trial_end'] == True) & (current_df['paid_TF'] == True)] # <- no records

current_df.loc[(current_df['cancel_before_trial_end'] == False) & (current_df['paid_TF'] == False)] # <- 11507 columns

current_df.loc[(current_df['cancel_before_trial_end'] == False) & (current_df['paid_TF'] == False),'initial_credit_card_declined'].value_counts()
'''
lots declined
True     11123
False      384
'''
# #==== drop those not decline 'error' data
# current_df = current_df.loc[(current_df['cancel_before_trial_end'] == False) & (current_df['paid_TF'] == False)\
#                             & (current_df['initial_credit_card_declined'] == False) == False]

subscribers = subscribers.loc[current_df.index]
subscribers

"""#### decribe data"""

subscribers.reset_index(drop = True, inplace = True)
subscribers

#========= check the type of each attributes
subscribers.dtypes

'''
subid                                    int64
package_type                            object
num_weekly_services_utilized           float64
preferred_genre                         object
intended_use                            object
weekly_consumption_hour                float64
num_ideal_streaming_services           float64
retarget_TF                               bool
age                                    float64
male_TF                                 object
country                                 object
attribution_technical                   object
attribution_survey                      object
op_sys                                  object
months_per_bill_period                   int64
plan_type                               object
monthly_price                          float64
discount_price                         float64
account_creation_date           datetime64[ns]
creation_until_cancel_days             float64
cancel_before_trial_end                   bool
trial_end_date                  datetime64[ns]
initial_credit_card_declined              bool
revenue_net                            float64
join_fee                               float64
language                                object
paid_TF                                   bool
refund_after_trial_TF                     bool
payment_type                            object
'''

#========= create the list of dtype attri
attri_ob = list(subscribers.dtypes[subscribers.dtypes == 'object'].index)
attri_fl = list(subscribers.dtypes[subscribers.dtypes == 'float64'].index)

#========= see the remaining columns
set(subscribers.columns).difference(set(attri_fl + attri_ob))

'''
{'account_creation_date',
 'cancel_before_trial_end',
 'initial_credit_card_declined',
 'months_per_bill_period',
 'paid_TF',
 'refund_after_trial_TF',
 'retarget_TF',
 'subid',
 'trial_end_date'}
'''

print(attri_ob)

#========= describe the data of all float attri
subscribers.describe()

subscribers.sort_values(by='creation_until_cancel_days').tail(30)

sns.pairplot(subscribers[attri_fl])

#========= describe the data of all object attri

pd.set_option('display.max_rows',None)
pd.DataFrame(subscribers[attri_ob].apply(pd.value_counts,axis=0).T.stack(),columns=['counts']).reset_index().\
        sort_values(by=['level_0','counts'],ascending=[True,False])

#========= Check how many attri have nan value

#print(list(subscribers.isnull().any()[subscribers.isnull().any() == True].index))
attri_nan = list(subscribers.isnull().any()[subscribers.isnull().any() == True].index)
"""

['package_type', 'num_weekly_services_utilized', 'preferred_genre', 'intended_use', 
'weekly_consumption_hour', 'num_ideal_streaming_services', 'age', 'male_TF', 'attribution_survey', 
'op_sys', 'creation_until_cancel_days', 'revenue_net', 'join_fee', 'payment_type']

"""

#========= Check the portion of nan value for each attri
subscribers[attri_nan].isnull().apply(lambda x: sum(x) / len(x),axis=0).sort_values(ascending=False)

"""#### missing and weird value"""

s1 = subscribers.copy()
s1

#========= describe the data of all float attri
subscribers.describe()

subscribers = s1.copy()

#====== flip 'cancel_before_trial_end'
subscribers['cancel_before_trial_end'] = subscribers['cancel_before_trial_end'] == False

#====== num_weekly_services_utilized (self reported)
subscribers.loc[subscribers['num_weekly_services_utilized'].isnull()] # <- missing value: 110450 (48%)

#====== weekly_consumption_hour (self reported): some negative and some missing
# subscribers.loc[subscribers['weekly_consumption_hour'] < 0] # <- error data: 29 rows drop
subscribers = subscribers.loc[(subscribers['weekly_consumption_hour'] >= 0) | subscribers['weekly_consumption_hour'].isnull()] # <- drop error data

# subscribers.loc[subscribers['weekly_consumption_hour'].isnull()] # <- missing value: 37930 rows (16%)

#====== num_ideal_streaming_services (self reported): # some negative and lots missing
# subscribers.loc[subscribers['num_ideal_streaming_services'] < 0] # <- error data: 1 row
subscribers = subscribers.loc[(subscribers['num_ideal_streaming_services'] >= 0) | (subscribers['num_ideal_streaming_services'].isnull())] # <- drop error data

# subscribers.loc[subscribers['num_ideal_streaming_services'].isnull()] # <- missing value: 112170 rows, 49% 

#====== age (self reported): # weird format, lots missing
# subscribers.loc[subscribers['age'].isnull()] # <- missing value: 35169 rows, 15%

#=== lots of weird formate in age, we can remedy this
index = subscribers.loc[(subscribers['age'] >1800) & (subscribers['age'] <= 2020)].index 
subscribers.loc[index,'age'] = (2020 - subscribers.loc[index,'age']).astype('int')

# subscribers.loc[subscribers['age'] > 200] # <- still weird data: 82 rows
subscribers = subscribers.loc[(subscribers['age'] < 200) | (subscribers['age'].isnull())]  # <- drop these rows
subscribers

#====== check the rest floating attri
#====== check monthly_price
subscribers['monthly_price'].value_counts()
'''
4.7343    226988
5.1013       325
1.0643       167
4.4407        18
4.3673        12
4.0003         2
1.1744         1
0.8074         1
4.6976         1
'''
#====== check discount_price
subscribers['discount_price'].value_counts()
'''
4.5141    226983
5.0279       325
1.0276       167
4.2205        18
4.0737        12
4.3673         4
4.4407         2
3.7801         2
1.1744         1
0.7707         1
'''

#====== check creation_until_cancel_days
subscribers.loc[subscribers['creation_until_cancel_days'] < 0] # <- weird data: 4 rows
subscribers = subscribers.loc[(subscribers['creation_until_cancel_days'] >= 0)| (subscribers['creation_until_cancel_days'].isnull())] # <- drop weird data
subscribers

#====== check revenue_net

subscribers.loc[subscribers['revenue_net'].isnull()] # <- missing value: 34904 rows

# subscribers.loc[subscribers['revenue_net'].isnull(),'join_fee'].value_counts() # <- null, means no revenue_net, no join_fee
# subscribers.loc[subscribers['join_fee'].isnull(),'revenue_net'].value_counts() # <- null

# subscribers.loc[subscribers['revenue_net'].isnull(),'paid_TF'].value_counts() # <- all True


subscribers

'''
payment_type                    0.595612
num_ideal_streaming_services    0.492778
num_weekly_services_utilized    0.485222
creation_until_cancel_days      0.333263
weekly_consumption_hour         0.166632
preferred_genre                 0.159585
package_type                    0.156281
age                             0.154502
join_fee                        0.153338
revenue_net                     0.153338
op_sys                          0.058758
intended_use                    0.015591
attribution_survey              0.011615
male_TF                         0.001182
'''

#=========================
#====== check those category attri
#====== package_type
subscribers['package_type'].value_counts()
'''
base        111408
enhanced     63213
economy      17336
'''
#== check missing value
subscribers.loc[subscribers['package_type'].isnull()] # <- 35553 rows
# subscribers.loc[subscribers['package_type'].isnull(),'preferred_genre'].value_counts() # <- very few have values, most also missing preferred_genre
# subscribers.loc[subscribers['package_type'].isnull(),'intended_use'].value_counts() # <- seem to be very few 'replace OTT'

'''
expand regional access         13834
expand international access    12792
education                       5062
other                           1313
replace OTT                        1
'''
# subscribers.loc[:,'intended_use'].value_counts() 
# <- seem that all id with 'intended_use' in ['expand regional access','expand international access','education'] dont have package_type & preferred_genre
'''
access to exclusive content    88985
replace OTT                    69156
supplement OTT                 26592
expand regional access         14018
expand international access    12973
other                           7107
education                       5136
'''

#====== preferred_genre
# subscribers.loc[subscribers['preferred_genre'].isnull()] # <- 36304 rows 16% the same as package_type

#=========================


#====== intended_use self_report
# subscribers.loc[subscribers['intended_use'].isnull()] # <- missing data: 3543 drop
subscribers = subscribers.loc[subscribers['intended_use'].isnull() == False] # <- drop missing data

#====== male_TF

# subscribers.loc[subscribers['male_TF'].isnull()] # <- missing data: 235 drop
subscribers = subscribers.loc[subscribers['male_TF'].isnull() == False] # <- drop missing data

#====== country

# subscribers['country'].value_counts() # <- all 'UAE', drop this columns
# subscribers.drop(columns = ['country'],inplace = True)


#====== attribution_survey
# subscribers.loc[subscribers['attribution_survey'].isnull()] # <- missing value: 550 rows
subscribers = subscribers.loc[subscribers['attribution_survey'].isnull() == False] # <- drop

#====== op_sys
#====== as it is not self-reported, might be some system which could not be recognized
# subscribers.loc[subscribers['op_sys'].isnull()] # <- missing value: 12842 rows

subscribers.loc[subscribers['op_sys'].isnull(),'op_sys'] = 'Others' # <- fill these samples with 'op_sys' = 'Others

#====== plan_type
subscribers['plan_type'].value_counts()

'''
base_uae_14_day_trial    222835
high_uae_14_day_trial       322
high_sar_14_day_trial        10
base_eur_14_day_trial         9
low_gbp_14_day_trial          4
high_aud_14_day_trial         2
'''
subscribers.loc[subscribers['plan_type'].isnull()] # <- nothing

#====== language
# subscribers['language'].value_counts() # <- all 'ar', drop this column
# subscribers.drop(columns = ['language'],inplace = True)

#====== payment_type

subscribers['payment_type'].value_counts()

'''
Standard Charter    37897
Paypal              30258
RAKBANK             14495
CBD                  4962
Najim                2361
Apple Pay               4
'''

subscribers.loc[subscribers['payment_type'].isnull()] # <- missing value: 133205 rows

"""### engagement"""

Image(filename = 'graph/engagement.jpg', width=800, height=200)

#========= Look at data

engagement

#========= describe the data of all float attri
engagement.describe()

#========= Check how many attri have nan value

attri_nan = list(engagement.isnull().any()[engagement.isnull().any() == True].index)

'''
['app_opens', 'cust_service_mssgs', 'num_videos_completed',
       'num_videos_more_than_30_seconds', 'num_videos_rated',
       'num_series_started']

'''
#========= Check the portion of nan value

engagement[attri_nan].isnull().apply(lambda x: sum(x) / len(x),axis=0).sort_values(ascending=False)

'''
num_series_started                 0.013385
num_videos_rated                   0.013385
num_videos_more_than_30_seconds    0.013385
num_videos_completed               0.013385
cust_service_mssgs                 0.013385
app_opens                          0.013385
'''

#========= Hypothesis: Might be some specific customers have no records on all columns

# engagement.loc[engagement['num_series_started'].isnull() == False].isnull().sum()

'''
subid                              0
date                               0
app_opens                          0
cust_service_mssgs                 0
num_videos_completed               0
num_videos_more_than_30_seconds    0
num_videos_rated                   0
num_series_started                 0
payment_period                     0
'''
#========= Hypothesis has been confirmed

engagement_id = engagement['subid'].unique()

#========= Create a new df without nan value

engagement_1 = engagement.loc[engagement['num_series_started'].isnull() == False]

engagement_1.head()

#========= Calculate the deltatime of each id

duration =  engagement_1.groupby(by = 'subid').date.apply(lambda x: (max(x) - min(x)).days + 1)

engagement_agg = pd.DataFrame(duration)

engagement_agg.columns = ['duration']

#========= Calculate the activating duration of each id

engagement_agg['activating_day'] = engagement_1.groupby(by = 'subid').date.agg(len)

#========= Calculate the max of pay_period of each id

engagement_agg['max_period'] = engagement_1.groupby(by = 'subid').payment_period.agg(max)

#========= Aggregate other feature and take sum
data = engagement_1[['subid','app_opens','cust_service_mssgs','num_videos_completed','num_videos_more_than_30_seconds',\
              'num_videos_rated','num_series_started',]].groupby(by = 'subid').agg(sum)
              
engagement_agg = pd.merge(left=engagement_agg,right=data,left_index=True,right_index=True)

engagement_agg

#========= Calculate some ratio (sum of aggragation data devided by activating_day)

engagement_agg_1 = engagement_agg[['app_opens',
       'cust_service_mssgs', 'num_videos_completed',
       'num_videos_more_than_30_seconds', 'num_videos_rated',
       'num_series_started']].apply(lambda x: x / engagement_agg['activating_day'],axis = 0)

engagement_agg_1 = pd.merge(left=engagement_agg[['duration','activating_day']],right=engagement_agg_1,left_index=True,right_index=True)


engagement_agg_1



"""### customer_service_reps"""

Image(filename = 'graph/customer service reps.jpg', width=800, height=250)

#========= Look at data
customer_service_reps.head(10)

#========= Check how many attri have nan value

attri_nan = list(customer_service_reps.isnull().any()[customer_service_reps.isnull().any() == True].index)

'''
['cancel_date', 'next_payment', 'renew']
'''

#========= Check the id and check the difference between ids
customer_service_reps_id = customer_service_reps['subid'].unique()

#========= compare engagement and subsceibers
subscribers_id = subscribers['subid']
engagement_id = engagement['subid'].unique()


# len(set(engagement_id)) # 92609

# len(set(subscribers_id)) # 227628

# len(set(customer_service_reps_id)) # 1369360

# len(set(engagement_id).difference(set(subscribers_id))) # 0 means engagement_id all belongs to subscribers_id

# len(set(subscribers_id).difference(set(customer_service_reps_id))) # 92578 means not all subscribers_id could be found in customer_service_reps_id

# len(set(engagement_id).difference(set(customer_service_reps_id))) # 0 means engagement_id all belongs to customer_service_reps_id

see = customer_service_reps.groupby(by='subid').num_trial_days.agg('first')
see.value_counts()

#========= Check the meaning of the attributes 'trial_completed_TF'
customer_service_reps.loc[customer_service_reps['trial_completed_TF'] == False]

# len(customer_service_reps.loc[customer_service_reps['trial_completed_TF'] == False]['subid'].unique())  # 660927

#========= Merge csr and subscribers tables to check the features

customer_service_reps_1 = customer_service_reps.loc[customer_service_reps['subid'].isin(subscribers_id)]
customer_service_reps_1.drop_duplicates(subset=['subid'],keep='first',inplace = True)
customer_service_reps_1 = pd.merge(customer_service_reps_1,subscribers,on='subid')

customer_service_reps_1[[ 'subid', 'current_sub_TF', 'cancel_date',
       'account_creation_date_x', 'num_trial_days', 'trial_completed_TF',
       'revenue_net_1month','last_payment', 'next_payment', 'renew',
       'retarget_TF',  'plan_type', 'account_creation_date_y',
       'creation_until_cancel_days', 'cancel_before_trial_end',
       'trial_end_date', 'initial_credit_card_declined', 
       'revenue_net','paid_TF', 'refund_after_trial_TF',
       'payment_type']]


#====== check relationship between 'trial_completed_TF' and 'cancel_before_trial_end'
# sum(customer_service_reps_1['trial_completed_TF'] == customer_service_reps_1['cancel_before_trial_end'])  # 0
# means 'trial_completed_TF' equals to 'cancel_before_trial_end'

#====== further check
# len(customer_service_reps.loc[customer_service_reps['payment_period'] == 1]) # 581391
# len(customer_service_reps.loc[(customer_service_reps['payment_period'] == 1) & (customer_service_reps['trial_completed_TF'] == True)]) # 581391
#= 'trial_completed_TF' could be seen as the signal of convert


customer_service_reps_1

#========= check the conversion rate in merged table
# sum(customer_service_reps_1['cancel_before_trial_end']) / len(customer_service_reps_1) # 0.5380068443430468

#========= Check logitics behind data

#==== First type: doesn't trial and diretly buy and suddenly quit, drop them
drop_id = customer_service_reps.loc[(customer_service_reps['payment_period']==1) & (customer_service_reps['trial_completed_TF'] == False),'subid'] # <- weird data: 13 rows
customer_service_reps = customer_service_reps.loc[customer_service_reps['subid'].isin(drop_id) == False] # <- drop
customer_service_reps

#==== Check those  doesn't trial and diretly buy
# customer_service_reps.loc[(customer_service_reps['num_trial_days']==0) & (customer_service_reps['trial_completed_TF']==False)] # <- 1442 rows
customer_service_reps.loc[(customer_service_reps['num_trial_days']==0)] # <- 79958 rows

#========= Check the distribution of different kind of trial on a time series

#==== step one, suppose the signal of conversion is decided by 'trial_completed_TF' in the first payment_period

customer_service_reps_2 = customer_service_reps.drop_duplicates(subset = ['subid'], keep = 'first')
customer_service_reps_2.set_index(keys = 'last_payment',inplace = True)
outcome = customer_service_reps_2.groupby(by = [pd.Grouper(freq='1W'),'num_trial_days']).agg({'payment_period':len,'trial_completed_TF':sum})
outcome

"""## Question 1: AB Testing"""

#========= Compare trial 7 days and trial 14 days, suppose the signal of conversion is payment_period = 1
#==== step one, drop num_trial_days == 0
customer = customer_service_reps.loc[(customer_service_reps['num_trial_days'] == 7) | (customer_service_reps['num_trial_days'] == 14)]

#==== step two, calculate number of data of trial 7 days and 14 days
customer_1 = customer.drop_duplicates(subset=['subid'],keep='first') 
n_7 = len(customer_1.loc[customer_1['num_trial_days'] == 7]) # 64043 for 7 days
n_14 = len(customer_1.loc[customer_1['num_trial_days'] == 14]) # 1281127 for 14 days

#==== step three, calculate conversion rate trial 7 days and 14 days
#== number of converted
len(customer.loc[(customer['payment_period'] == 1) & (customer['num_trial_days'] == 7)]) # 35314 for 7days
len(customer.loc[(customer['payment_period'] == 1) & (customer['num_trial_days'] == 14)]) # 523596 for 14 days

#== conversion rate
p_7 = 35314 / 64043 # 0.5514107708883094 for 7 days
p_14 = 523596 / 1281127 # 0.4086995278375992 for 14 days

customer.loc[customer['payment_period'] == 0].groupby(by='num_trial_days')['trial_completed_TF'].agg(len)

#========= Validate Hypothesis
#==== Null Hypothesis: p_7 hat = p_14 hat, the length of trial doesn't matter
#==== Alternative Hypothesis: the length of trial does matter
z = (p_7 - p_14) / math.sqrt((p_7 * (1 - p_7) / n_7 + p_14 * (1 - p_14) / n_14)) # 70.90511117706859

#=== adopt 95% two tail test, with z = 1.96

70.9 > 1.96 # thus reject Null Hypothesis

"""## Question 3: Advertising"""

#========= Look at data
advertising_spend

#========= Check the category of channel

attribution = subscribers[['account_creation_date','attribution_technical','attribution_survey']]
# attribution.set_index(keys = 'subid',drop = True, inplace = True)
pd.DataFrame(attribution[['attribution_technical','attribution_survey']].apply(pd.value_counts,axis=0).T.stack(),columns=['counts']).reset_index().\
        sort_values(by=['level_0','counts'],ascending=[True,False])

data = attribution['attribution_survey'].value_counts()
data
labels = data.index
# sizes = data
explode = (0.1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
# plt.pie(sizes,explode=explode,labels=labels,autopct='%1.1f%%',shadow=False,startangle=150)
plt.style.use("seaborn-notebook")
fig, ax = plt.subplots(figsize=(15, 15))
ax.legend(fontsize=20)
# ax.set_xlabel(fontsize=20)
plt.rcParams.update({'font.size': 15})
ax.pie(data,labels=labels,explode=explode,autopct='%1.1f%%',shadow=False,startangle=150,textprops= {'fontsize':15})
plt.title("attribution_survey")
plt.show()

data = attribution['attribution_technical'].value_counts()
len(data)
labels = data.index
# sizes = data
explode = (0.1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
len(explode)
# plt.pie(sizes,explode=explode,labels=labels,autopct='%1.1f%%',shadow=False,startangle=150)
plt.style.use("seaborn-notebook")
fig, ax = plt.subplots(figsize=(15, 15))
ax.legend(fontsize=20)
# ax.set_xlabel(fontsize=20)
plt.rcParams.update({'font.size': 15})
ax.pie(data,labels=labels,explode=explode,autopct='%1.1f%%',shadow=False,startangle=150,textprops= {'fontsize':15})
plt.title("attribution_survey")
plt.show()

#========= Look at attribution data
attribution

attribution

organic_list = ['organic','google_organic','facebook_organic','bing_organic','pinterest_organic']

attribution['attribution_technical_new'] = attribution['attribution_technical']
attribution.loc[attribution['attribution_technical_new'].isin(organic_list),'attribution_technical_new'].index

#========= Conduct allocation
#==== Suppose the account_creation_date could be seen as a signal
#==== Step one: If organic in technical, then the same as survey
organic_list = ['organic','google_organic','facebook_organic','bing_organic','pinterest_organic']

attribution['attribution_technical_new'] = attribution['attribution_technical']
index = attribution.loc[attribution['attribution_technical_new'].isin(organic_list),'attribution_technical_new'].index
attribution.loc[index,'attribution_technical_new'] = attribution.loc[index,'attribution_survey']

#==== Step two: Fill the nan data in survey and technical
# index = attribution['attribution_technical_new'].isnull().index
index = attribution.loc[attribution['attribution_technical_new'].isnull()].index
attribution.loc[index,'attribution_technical_new'] = attribution.loc[index,'attribution_survey']

# index = attribution['attribution_survey'].isnull().index
index = attribution.loc[attribution['attribution_survey'].isnull()].index
attribution.loc[index,'attribution_survey'] = attribution.loc[index,'attribution_technical_new']

attribution.fillna(value='other',inplace = True)

attribution

#========= Continue allocation
#==== Calculate the monthly advertising outcome
survey_attribution = attribution.resample('M',on = 'account_creation_date')['attribution_survey'].apply(pd.value_counts).unstack()
technical_attribution = attribution.resample('M',on = 'account_creation_date')['attribution_technical_new'].apply(pd.value_counts).unstack()

#==== fill the lost three channel in survey_attribution
lost_df = pd.DataFrame(data=0,columns=['email_blast', 'email', 'brand sem intent google'],index=survey_attribution.index)
survey_attribution = pd.merge(left=survey_attribution, right=lost_df, left_index=True, right_index=True)

#==== quote the columns needed
advertising_spend.set_index(keys='date', drop=True, inplace = True)

survey_attribution = survey_attribution[advertising_spend.columns]
technical_attribution = technical_attribution[advertising_spend.columns]



total_attribution = survey_attribution * 0.5 + technical_attribution * 0.5

total_attribution

CAC_df = advertising_spend / total_attribution
CAC_df

from matplotlib import rcParams, cycler

plt.style.use("seaborn-notebook")

data = CAC_df
data = np.array(data).T
cmap = plt.cm.coolwarm
rcParams['axes.prop_cycle'] = cycler(color=cmap(np.linspace(0, 1,)))

fig, ax = plt.subplots(figsize=(15, 6))
lines = ax.plot(CAC_df)

plt.ylabel('CAC')
plt.xlabel('Time')
plt.title('CAC of each channel on a time_series',fontdict={'size':20})
ax.legend(lines,CAC_df.columns)

from matplotlib import rcParams, cycler

plt.style.use("seaborn-notebook")

data_ = CAC_df[['facebook', 'search', 'brand sem intent google', 'affiliate',
       'email_blast', 'pinterest', 'referral']]
data_ = data_.iloc[1:]
data = np.array(data).T
cmap = plt.cm.coolwarm
rcParams['axes.prop_cycle'] = cycler(color=cmap(np.linspace(0, 1, N)))

fig, ax = plt.subplots(figsize=(15, 6))
lines = ax.plot(data_)

plt.ylabel('CAC')
plt.xlabel('Time')
plt.title('CAC of each channel on a time_series',fontdict={'size':20})
ax.legend(lines,data_.columns)

CAC_df.columns

"""## Churn model"""

customer_service_reps

subscribers

engagement_1

engagement_agg_1

"""#### preparation"""

subscribers.set_index( keys='subid',drop = True, inplace = True)
df = pd.merge(left = subscribers, right = engagement_agg_1, left_index = True, right_index = True,how = 'inner')
df

# ====== extract label info from cutomer_service_rep
#==== the sacle is those who have successfully gone through to period 1
customer_with_period_one = customer_service_reps.loc[customer_service_reps['payment_period'] == 1,'subid']
customer_service_reps_1 = customer_service_reps.loc[customer_service_reps['subid'].isin(customer_with_period_one)]
customer_service_reps_2 = customer_service_reps_1.loc[customer_service_reps_1['next_payment'].isnull()]
customer_service_reps_2 = customer_service_reps_2[['subid','current_sub_TF','num_trial_days',
                            'billing_channel','revenue_net_1month','payment_period']]
customer_service_reps_2.set_index(keys='subid',drop = True, inplace = True)
customer_service_reps_2

#========= Merge two df
df = pd.merge(left = df, right = customer_service_reps_2, left_index = True, right_index = True, how = 'inner')

#========= Check how many attri have nan value

#print(list(subscribers.isnull().any()[subscribers.isnull().any() == True].index))
attri_nan = list(df.isnull().any()[df.isnull().any() == True].index)
attri_nan
"""

['package_type', 'num_weekly_services_utilized', 'preferred_genre', 'intended_use',
 'weekly_consumption_hour', 'num_ideal_streaming_services', 'age', 'attribution_survey', 'op_sys',
 'creation_until_cancel_days', 'revenue_net', 'join_fee']

"""

#========= Check the portion of nan value for each attri
df[attri_nan].isnull().apply(lambda x: sum(x) / len(x),axis=0).sort_values(ascending=False)

df

#====== choose columns for models
df_missing_value = df[['package_type', 'num_weekly_services_utilized', 'preferred_genre', 'weekly_consumption_hour',
 'num_ideal_streaming_services', 'age', 'creation_until_cancel_days', 'revenue_net', 'join_fee']]

#====== filter those who request for refund, as they should not be counted as complete trail
df = df.loc[df['refund_after_trial_TF'] == False]

df_1 = df.drop(columns = ['num_weekly_services_utilized', 'weekly_consumption_hour',\
 'num_ideal_streaming_services', 'age', 'creation_until_cancel_days', 'revenue_net', 'join_fee','country','account_creation_date'\
 ,'trial_end_date','language','paid_TF','refund_after_trial_TF','plan_type','attribution_technical','attribution_survey'])


#====== fillna
df_1.fillna(value = 'Others',inplace = True)

#====== get dummy
df_2 = pd.get_dummies(df_1, prefix=['package_type', 'preferred_genre','intended_use','op_sys','payment_type','billing_channel'])
df_2

#====== create churn label
df_2['Churn_TF'] = 1 - df_2['current_sub_TF']
df_2.drop(columns = ['current_sub_TF'],inplace = True)

df_2

"""#### modelling"""

#====== import package
from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV

from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix

from hyperopt import Trials, STATUS_OK, tpe, hp, fmin

import xgboost as xgb
from xgboost import XGBClassifier

from tqdm import tqdm

from imblearn.combine import SMOTEENN

from sklearn.linear_model import LogisticRegression as LR

"""#### Logistic Regression"""

#====== train test split
X = df_2.drop(columns = ['Churn_TF'])
y = df_2['Churn_TF']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

#====== train model and turn model via Gridsearch
param_test = {'C':[5 ** i for i in range(-5,5)],'penalty':['l1','l2']}

gsearch = GridSearchCV(estimator = LR(random_state = 1),
   param_grid = param_test, scoring='roc_auc', cv=5)

gsearch.fit(X_train,y_train)
gsearch.cv_results_['mean_test_score'], gsearch.best_params_, gsearch.best_score_

#====== Fitting LR to the Training set
classifier = LR(C = 5, penalty = 'l2')


#====== Applying k-Fold Cross Validation and see the outcome
acc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 5, scoring='roc_auc')
CrossValMean = acc.mean()
print("Final CrossValMean: ", CrossValMean)

CrossValSTD = acc.std()
print("Final CrossValstd: ", CrossValSTD)

#====== Make prediction and output confusion matrix
clf = classifier.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print(accuracy_score(y_test,y_pred))
print(classification_report(y_test, y_pred))

#====== See the coefficient of features(those improve probility of churn)
pd.Series(clf.coef_[0],index=X.columns).sort_values(ascending=False).head(10)

#====== See the coefficient of features(those decrease probility of churn)
pd.Series(clf.coef_[0],index=X.columns).sort_values(ascending=True).head(10)

#====== draw auc curve
from sklearn.metrics import roc_curve
y_pred = clf.predict_proba(X_test.values)
fpr, tpr, thresholds = roc_curve(y_test, y_pred[:,1])

fig, ax = plt.subplots(figsize=(5, 5))
ax.legend(fontsize=20)

plt.plot(fpr,tpr,)
# ax.set_xlabel(fontsize=20)
# plt.rcParams.update({'font.size': 15})
# ax.pie(data,labels=labels,explode=explode,autopct='%1.1f%%',shadow=False,startangle=150,textprops= {'fontsize':15})
plt.title("auc curve",fontsize = 15)
plt.show()

y_pred[:,1]

y_pred_ = y_pred[:,1] > 0.9

outcome = pd.DataFrame(y_test)
outcome.columns = ['y_test']
outcome['y_pred'] = y_pred_
outcome

churn_churn = len(outcome.loc[(outcome['y_test'] == 1) & (outcome['y_pred'] == 1)]) / len(outcome)
churn_renew = len(outcome.loc[(outcome['y_test'] == 0) & (outcome['y_pred'] == 1)]) / len(outcome)
renew_churn = len(outcome.loc[(outcome['y_test'] == 1) & (outcome['y_pred'] == 0)]) / len(outcome)
renew_renew = len(outcome.loc[(outcome['y_test'] == 0) & (outcome['y_pred'] == 0)]) / len(outcome)

print (churn_churn,churn_renew, renew_churn, renew_renew)

"""#### Try xgboost"""

#====== modelling with XGBoost
#====== mainly use hyperopt to conduct model tuning

def objective(space):

#=== choose XGBoost
    classifier = xgb.XGBClassifier(n_estimators = space['n_estimators'],
                            max_depth = int(space['max_depth']),
                            learning_rate = space['learning_rate'],
                            gamma = space['gamma'],
                            min_child_weight = space['min_child_weight'],
                            subsample = space['subsample'],
                            colsample_bytree = space['colsample_bytree']
                            )

    # applying k-Fold Cross Validation to optimize roc_auc
    acc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 5, scoring='roc_auc')
    CrossValMean = acc.mean()

    return{'loss':1-CrossValMean, 'status': STATUS_OK }

#=== decide the range of params
space = {
    'max_depth' : hp.choice('max_depth', range(5, 30, 1)),
    'learning_rate' : hp.quniform('learning_rate', 0.01, 0.5, 0.01),
    'n_estimators' : hp.choice('n_estimators', range(20, 205, 5)),
    'gamma' : hp.quniform('gamma', 0, 0.50, 0.01),
    'min_child_weight' : hp.quniform('min_child_weight', 1, 10, 1),
    'subsample' : hp.quniform('subsample', 0.1, 1, 0.01),
    'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1.0, 0.01)}

trials = Trials()
best = fmin(fn=objective,
            space=space,
            algo=tpe.suggest,
            max_evals=50,
            trials=trials)
#=== print the best outcome
print("Best: ", best)

classifier = XGBClassifier(n_estimators = best['n_estimators'],
                            max_depth = best['max_depth'],
                            learning_rate = best['learning_rate'],
                            gamma = best['gamma'],
                            min_child_weight = best['min_child_weight'],
                            subsample = best['subsample'],
                            colsample_bytree = best['colsample_bytree']
                            )

#====== Applying k-Fold Cross Validation and see the outcome
acc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 5, scoring='roc_auc')
CrossValMean = acc.mean()
print("Final CrossValMean: ", CrossValMean)

CrossValSTD = acc.std()
print("Final CrossValstd: ", CrossValSTD)

#====== Make prediction and output confusion matrix
clf = classifier.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print(accuracy_score(y_test,y_pred))
print(classification_report(y_test, y_pred))

"""##CLV"""

#====== generate prob for each customer

clf = classifier.fit(X, y)
y_pred = clf.predict_proba(X)

df_3 = df_2.copy()
df_3['Churn_prob'] = y_pred[:,1]

#====== find the group
df_3 = df_3.loc[df_3['Churn_TF'] == 0]
df_3 = df_3[['discount_price','revenue_net_1month','Churn_prob']]
df_3

#====== bring back the cac info to calculate clf
df_3['account_creation_date'] = subscribers.loc[df_3.index,'account_creation_date']

attribution = attribution[['attribution_survey','attribution_technical_new']]

df_3 = pd.merge(left = df_3, right = attribution, left_index = True, right_index = True, how = 'inner')

df_3

CAC_df

from datetime import timedelta
df_3['format_date'] = pd.to_datetime(df_3['account_creation_date'],format='%Y-%m')
df_3['format_date'] = df_3['format_date'].apply(lambda x: x.strftime('%Y-%m'))
df_3['format_date'] = pd.to_datetime(df_3['format_date'],format = '%Y-%m')
df_3

CAC_df_1 = CAC_df.copy()
CAC_df_1['time'] = CAC_df_1.index
CAC_df_1['time']= CAC_df_1['time'].apply(lambda x: x.strftime('%Y-%m'))
CAC_df_1['time'] = pd.to_datetime(CAC_df_1['time'],format = '%Y-%m')
CAC_df_1.set_index(keys = 'time',drop = True, inplace = True)
CAC_df_1

col = CAC_df_1.columns.to_list()
col
def approach(array_like):
  time = array_like['format_date']
  mapping = CAC_df_1.loc[time]
  expense = 0

  survey = array_like['attribution_survey']
  technical = array_like['attribution_technical_new']
  if survey in col:
    expense += mapping[survey]
  if technical in col:
    expense += mapping[technical]
  return expense

expenses = df_3.apply(lambda x: approach(x),axis = 1)
df_3['expenses'] = expenses

df_3 = df_3[['discount_price','revenue_net_1month','expenses','Churn_prob']]
df_3

# present value of revenue_net_1month (considering joint fee) + present value of annuity (discount_price) - expenses(CAC)
# adopt 10% annual discount rate
# equalvant monthly discount rate is 0.8%
# df_3['CLV'] = df_3['revenue_net_1month'] / 1.004 + df_3['discount_price'] * (1 - df_3['Churn_prob']) / 0.01 - df_3['expenses']
df_3['CLV'] = df_3['revenue_net_1month'] / 1.004 + df_3['discount_price'] / (1 + 0.008 - 1 + df_3['Churn_prob']) - df_3['expenses']
df_3

df_3['CLV'].sum()

